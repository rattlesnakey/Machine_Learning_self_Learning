{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单一变量法进行特征选择，也就是根据相关性来进行特征选择的\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2654095a9a8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelectPercentile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#设置特征选择参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mselect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSelectPercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#也是先Fit 然后再transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train_selected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SO' is not defined"
     ]
    }
   ],
   "source": [
    "#导人特征选择工具\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "#设置特征选择参数\n",
    "select=SelectPercentile(percentile=50)#percentile设置50指的是设置为提取原来特征数量的一半的数量出来\n",
    "select.fit(X_train_scaled, y_train)#也是先Fit 然后再transform \n",
    "X_train_selected = select.transform(X_train_scaled)\n",
    "#打印特征选择结果\n",
    "print (\"经过缩放的特征形态{}\". format(X_train_scaled.shape))\n",
    "print (\"特征选择后的特征形态\". format(X_train_selected.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据中有很多噪声的时候就需要feature selection一下，效果会更好\n",
    "\n",
    "\n",
    "selectpercentile和selectkBest 其实是卡方检验的一种方式\n",
    "https://7125messi.github.io/post/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0/\n",
    "卡方检验中的CHI值越大说明两个变量越不可能独立，那么就相关，反正总的来说还是去检验两者的相关性之类的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有的是基于模型的特征选择\n",
    "selectfrommodel\n",
    "你先定义一个模型，然后把这个模型的实例作为slectfrommodel的参数\n",
    "\n",
    "\n",
    "＃导人基于模型的特征选择工具\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "＃导入随机森林模型\n",
    "from sklearn . ensemble import RandomForestRegressor\n",
    "＃设置模型n estimators 参数\n",
    "sfm = SelectFromModel (RandomForestRegressor(n estimators=lOO ,\n",
    "random state=38) ,\n",
    "threshold= ' median 『）\n",
    "＃使用模型拟合数据\n",
    "sfm . fit(X_train_scaled , y_train)\n",
    "X train sfm = sfm . transform(X train scaled)\n",
    "＃打印结果\n",
    "print （’ 基于随机森林模型迸行特征后的数据形态{} ’. format (X _train,_ sfm . shape ) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "从上面的代码可以看到，我们使用了随机森林回归模型来进行特征选择。为什么要\n",
    "使用随机森林昵？因为包括随机森林在内的基于决策树的算法都会内置一个称为feature\n",
    "importances一的属性，我们可以让SelectFromModel 直接从这个属性中抽取特征的重要性。\n",
    "当然除了随机森林之外，其他算法也是可以的，例如使用L1 正则化的线性模型，它们可\n",
    "以对数据空间的稀疏系数进行学习，从而可以当作特征重要性来抽取。原本这个系数是\n",
    "线性模型用来为自己建模的，我们也可以借助它来帮助其他模型进行数据预处理。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "get_support()可以用来看有哪些特征被选择了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "还有迭代式特征选择\n",
    "\n",
    "\n",
    "顾名思义，迭代式特征选择是基于若干个模型进行特征选择。在s cikit-learn中，有\n",
    "一个称为递归特征剔除法（Recurise Feature Elimination, RFE ）的功能就是通过这种方式\n",
    "来进行特征选择的。在最开始，会用某个模型对特征进行选择，之后再建立两个模型，\n",
    "其中一个对己经被选择的特征进行筛选：另外一个对被剔除的模型进行筛选，然后一直\n",
    "重复这个步骤，直到达到我们指定的特征数量。这种方式比前面我们学习的基于单个模\n",
    "型进行特征选择更加强悍，但是相应地，对计算能力的要求也更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导人RFE工具\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(RandomForestRegressor(n_estimators=100,\n",
    "random state=38) ,\n",
    "n_features_to_select=12)\n",
    "#使用RFE 工具拟合数据\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "#显示保留的特征\n",
    "mask= rfe.get_support()\n",
    "print (mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
